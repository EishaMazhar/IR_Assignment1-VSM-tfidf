{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from queue import LifoQueue \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from scipy import spatial\n",
    "import math\n",
    #"import tkinter\n",
    #"top = tkinter.Tk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking input from stop word list file\n",
    "\n",
    "fObj=open('StopwordList.txt','r')\n",
    "SwContent=fObj.readlines()\n",
    "swlist = [x.replace(\"\\n\",\"\").replace(\" \",\"\") for x in SwContent]\n",
    "swl = [x for x in swlist if x!=\"\"]           # stop word list stored in 'swl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldict={} #lowcase doc\n",
    "ldict1={} #without stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking input from file and preprocessing\n",
    "\n",
    "list=glob.glob('Speeches/*')\n",
    "doclist=[]\n",
    "\n",
    "for x in sorted(list):\n",
    "#     print(x)\n",
    "    f=open(x,'r')\n",
    "    \n",
    "    # read file and removing punctuations\n",
    "    fullfile=f.read().replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"'\",\"\").replace(\"]\",\" \").replace(\"[\",\"\").replace(\",\",\" \").replace(\"?\",\"\").replace(\"\\n\",\" \").replace(\"-\",\" \").replace(\":\",\" \").replace(\"$\",\" \").split() \n",
    "\n",
    "    # stemming and lower case conversion\n",
    "    lowCasedoc=[lemmatizer.lemmatize(x.lower()) for x in fullfile]\n",
    "    \n",
    "    #removing stop words\n",
    "    withoutstoplist = [x for x in lowCasedoc if x not in swl]\n",
    "    \n",
    "    #trimming the file name and removing redundant '.txt'\n",
    "    p=os.path.basename(x)\n",
    "    p=p.split('.')[0]\n",
    "    ldict[p]=lowCasedoc\n",
    "    ldict1[p]=withoutstoplist\n",
    "    doclist.append(p)  #maintaining a list of documents\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length without stopwords:\",len(ldict1['speech_0']))\n",
    "print(\"length with stopwords:\",len(ldict['speech_0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index creation\n",
    "term_count_in_each_doc={}\n",
    "pindex_table={}\n",
    "wfreq={}\n",
    "#iterating through docs\n",
    "for key in ldict1.keys():\n",
    "    TC=0       # maintain the total count of the terms and can tell their index during iteration\n",
    "    #iterating through words in each doc\n",
    "    for word in ldict1[key]:\n",
    "        TC+=1\n",
    "        if word in swl:  # entertaining the presence of stop words in the file (increment index without doing anything)\n",
    "            continue\n",
    "        if word not in pindex_table:\n",
    "            pindex_table[word]={}\n",
    "            pindex_table[word][key]=[]\n",
    "            pindex_table[word][key].append(TC) \n",
    "        else:\n",
    "            if key not in pindex_table[word]:\n",
    "                pindex_table[word][key]=[]\n",
    "            pindex_table[word][key].append(TC)\n",
    "            \n",
    "    term_count_in_each_doc[key]=TC     #total no of words in a doc (including stop words) for TTf\n",
    "    \n",
    "# print(len(pindex_table['our'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTFIDFDocsAndQuery(pindex_table, doclist,query):\n",
    "    tfidf_dict={}\n",
    "#     idf=log(N/no of documents with the term)\n",
    "    for word in pindex_table.keys():\n",
    "        tfidf_dict[word]={}\n",
    "        dfreq=0\n",
    "        #doc freq\n",
    "        dfreq=len(pindex_table[word])\n",
    "\n",
    "        #idf\n",
    "        idf=math.log(56/(dfreq))\n",
    "        tfidf_dict[word]=[]\n",
    "        for i in range(len(doclist)):\n",
    "            x='speech_{}'.format(i)\n",
    "            if(x in pindex_table[word].keys()):  #if term exist in document\n",
    "                tfidf_dict[word].append((len(pindex_table[word][x])*idf))\n",
    "            else:\n",
    "                tfidf_dict[word].append(0)      #if term doesn't exist in document\n",
    "        if word in query:\n",
    "            tfidf_dict[word].append(round(((query.count(word))*idf),4))\n",
    "        else:\n",
    "            tfidf_dict[word].append(0)\n",
    "            \n",
    "    print(tfidf_dict)\n",
    "                \n",
    "    return tfidf_dict\n",
    "    \n",
    "# # \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocAndQueryVector(tfidf_dict,doclist):\n",
    "#     print(tfidf_dict.keys())\n",
    "\n",
    "    docVec={}\n",
    "    \n",
    "    for docNo in range(len(doclist)):\n",
    "        \n",
    "        x='speech_{}'.format(docNo)\n",
    "        docVec[x]=[]\n",
    "        \n",
    "        for word in tfidf_dict.keys():\n",
    "            docVec[x].append(tfidf_dict[word][docNo])\n",
    "\n",
    "    queryVec=[]\n",
    "    \n",
    "    for word in tfidf_dict.keys():\n",
    "        queryVec.append(tfidf_dict[word][56])     \n",
    "            \n",
    "    return docVec, queryVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity function\n",
    "\n",
    "def cosineSimilarity(docVec, queryVec, c=0.005):\n",
    "    \n",
    "    cosineVector={}\n",
    "    cosineList=[]\n",
    "    for i in docVec.keys(): \n",
    "        result = round(1 - spatial.distance.cosine(docVec[i], queryVec),3)\n",
    "        cosineVector[i]=result\n",
    "        cosineList.append(result)\n",
    "        \n",
    "    sortedDocIds=sorted(range(len(cosineList)), key=cosineList.__getitem__, reverse=True)\n",
    "\n",
    "    counter=1\n",
    "    for i in sortedDocIds:\n",
    "        if(cosineList[i] > c):\n",
    "            print(counter,\"----> Doc\",i,\" ---->\",cosineList[i])\n",
    "            counter=counter+1\n",
    "            \n",
    "    print(cosineSimilarity)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=input(\"Enter the query: \")\n",
    "\n",
    "cutoff=float(input(\"Enter the cutoff value : \"))\n",
    "\n",
    "# removing punctuations from query\n",
    "query=query.replace(\".\",\"\").replace(\"n't\",\" not\").replace(\"]\",\" \").replace(\"[\",\" \").replace(\",\",\" \").replace(\")\",\"\").replace(\"?\",\"\").replace(\"/\",\"\").replace(\"(\",\"\").split()\n",
    "\n",
    "#stemming the query\n",
    "stemmedQuery=[lemmatizer.lemmatize(x.lower()) for x in query]\n",
    "\n",
    "QueryWithoutStoplist = [x for x in stemmedQuery if x not in swl]\n",
    "print(QueryWithoutStoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict={}\n",
    "tfidf_dict=calculateTFIDFDocsAndQuery(pindex_table,doclist,QueryWithoutStoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docVec, queryVec = DocAndQueryVector(tfidf_dict,doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarity(docVec, queryVec, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    #"top.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
